
\newpage

\section{Model Structure}

\subsection{General Description}

With the data acquired from HuggingFace, the process of making a machine learning model could begin. The model that was developed was a convolutional neural network (CNN). It accepts full-size images of shape $(512, 512, 3)$, with each image being an rgb image with values ranging from 0 to 255. \Cref{code:model_definition} shows the \code{src/model_def.py} file, which was called by the \code{main.ipynb} and \code{src/train.py} files to train and tune the model.

\begin{figure}[h]
    \lstinputlisting[language=Python, caption=Model definition code extract, label=code:model_definition]{code_chunks/sec_3_model.py}
\end{figure}

The architecture began with a \code{Conv2D} layer using a $3{\times}3$ kernel and a tunable number of filters $f_{1}$. The stride was set to the default value of 1, and padding was set to \emph{same} to preserve spatial dimensions, at the cost of increased computation. A \code{ReLU} activation function was applied due to its simplicity and effectiveness in addressing vanishing gradients.

This was followed by a pooling layer, with either max or average pooling selectable. By default, pooling used a $2{\times}2$ window with stride 2, therefore halving the spatial dimensions while retaining salient features.

A second convolutional block repeated this pattern with a $3{\times}3$ kernel and tunable filter count $f_{2}$, again followed by max or average pooling with the same parameters.

After the convolutional blocks, a global pooling layer removed spatial dimensions, with max or average pooling selectable. The dense head consisted of a flattening layer, followed by a fully connected layer with $d$ units and \code{ReLU} activation. An optional dropout layer with rate $p$ was included for regularisation. The final output layer was a single neuron with sigmoid activation, producing a probability for binary classification.

Adam was selected as one of the two selectable optimisers due to its rapid convergence and robustness to vanishing learning rates and high variance, making it widely used in practice \cite{RAIAAN2024100470}. Adagrad was used as the secondary selectable optimiser, as it eliminates the need for manual learning rate tuning and generally outperforms simpler methods like SGD or momentum-based optimisers. However, its performance may degrade over time due to a monotonically decreasing learning rate, which slows convergence \cite{RAIAAN2024100470}.

Finally, binary cross-entropy was used as the loss function, being most appropriate for binary classification. Each model was trained for 3 epochs on an \code{ml.c5.2xlarge} instance, prioritising cost-efficiency over training speed.


\newpage

\subsection{Hyperparameter Tuning with Hyperband}

Hyperparameter tuning was conducted to maximise model performance by iteratively adjusting key parameters. This process was performed on the small training set to enable faster experimentation, with the intention of applying the optimal hyperparameters to train a final model on the full training set.

\Cref{tab:tunable_hyperparameters} summarises the code snippets shown in \cref{code:hyperparameter_tuning}, listing the parameters tuned for the model along with the final values selected by the tuner. Given the size of the search space, an exhaustive grid search was not feasible. Instead, the Hyperband algorithm was used, leveraging early stopping to efficiently allocate resources to the most promising hyperparameter configurations \cite{hyperband}.

Following the initial tuning on the small training set, the best performing hyperparameters were extracted and applied to train the final model on the full training set. The beginning of this process is shown in \cref{code:hyperparameter_extraction}.


\begin{table}[h]
    \centering
    \caption{Tunable Hyperparameters and Final Chosen Values (see implementation snippets in \cref{code:hyperparameter_tuning})}
    \begin{tabular}{lll}
    \toprule
    \textbf{Hyperparameter} & \textbf{Range/Choices} & \textbf{Final Value} \\
    \midrule
    learning rate           & $1{\times}10^{-4}$ to $1{\times}10^{-2}$ (log scale) & 0.00149 \\
    dropout-rate            & $0.0$ to $0.5$ (used if \texttt{use-dropout=true}) & 0.284 \\
    conv1-filters ($f_1$)   & $16$ to $128$ & 24 \\
    conv2-filters ($f_2$)   & $32$ to $256$ & 107 \\
    dense-units ($d$)       & $64$ to $512$ & 254 \\
    pooling                 & \texttt{max}, \texttt{avg} & max \\
    use-dropout             & \texttt{true}, \texttt{false} & true \\
    optimizer               & \texttt{adam}, \texttt{adagrad} & adam \\
    \bottomrule
    \end{tabular}
    \label{tab:tunable_hyperparameters}
\end{table}

\begin{figure}[h]
    \lstinputlisting[language=Python, caption=Hyperparameter extraction code snippet, label=code:hyperparameter_extraction]{code_chunks/sec_3_hp_extraction.py}
\end{figure}

\begin{figure}[p]
    \lstinputlisting[language=Python, caption=Hyperparameter tuning code extracts from \code{main.ipynb} and \code{src/train.py}; used to call \code{src/model_def.py} (\cref{code:model_definition}), label=code:hyperparameter_tuning]{code_chunks/sec_3_hp_tuning.py}
\end{figure}

\newpage

\section{Model Deployment} \label{sec:model_deployment}

This model, along with the transfer learning model described later, in \cref{sec:transfer_learning}, was deployed using Amazon SageMaker, as illustrated in \cref{fig:endpoint}. The deployment process is detailed in \cref{code:deployment_endpoint}, which includes exception handling to initialise \code{main_model_predictor} only if it is not already present in memory. The same script also contains the inference logic used to generate predictions on the holdout set. Further discussion of the deployment workflow and challenges encountered is provided in the AWS reflection section (\cref{sec:model_deployment,sec:aws_reflection}).


\begin{figure}[h]
    \centering
    \includegraphics[width=200px]{figures/endpoints_screenshot.png} % Image filename
    \centering
    \caption{Endpoint page screenshot in Sagemaker, as evidence of deployment} % Caption
    \label{fig:endpoint} % Label
\end{figure}

\begin{figure}[h]
    \lstinputlisting[language=Python, caption=Model Deployment and inference code snippet, label=code:deployment_endpoint]{code_chunks/sec_4_example_deployment.py}    
\end{figure}

\newpage

\section{Transfer Learning} \label{sec:transfer_learning}

Several CNN architectures are available for transfer learning, as outlined in \cite{keras_applications}. EfficientNetV2 was selected for its strong balance between accuracy and computational efficiency on general image classification tasks, making it well suited to the diverse image content of the aiornot dataset \cite{keras_applications, tan2021efficientnetv2}.

EfficientNetB0, pre trained on ImageNet, was used as a feature extractor with its classification head removed and average pooling applied. The final 10\% of layers were unfrozen to enable fine-tuning. A dropout layer with rate $p$ was included for regularisation, followed by a dense output layer with sigmoid activation to produce a binary class probability. The model accepted inputs of shapes that were smaller than the original 500x500 due to issues with memory allocation during training. The \code{src/transfer_learning.py} script defined the model and training logic, and was invoked by the \code{transfer_learning.ipynb} notebook; selected code snippets are shown in \cref{code:transfer_learning_code}.

\begin{figure}[h]
    \lstinputlisting[language=Python, caption=Transfer Learning Code Snippet, label=code:transfer_learning_code]{code_chunks/sec_5_transfer_learning.py}
    \label{fig:transfer_learning_code}
\end{figure}

The model was then deployed to a SageMaker endpoint using the same strategy shown previously in \cref{code:deployment_endpoint}, and evaluated on precision, recall, F1 score, and accuracy.

\newpage

\section{Model Comparison And Evaluation}

This section compares the performance of the models trained on the dataset. Evaluation metrics were obtained by invoking the deployed endpoints on the holdout set, which was not used during training or validation. This approach ensures that performance is measured on unseen data, providing a more realistic indication of generalisation to real world use cases. The results of the model evaluation are summarised in \Cref{tab:model_comparison}


\begin{table}[h]
\centering
\caption{Model Comparison}
\begin{tabular}{rcccc}
\toprule
\textbf{Model} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F1 Score} \\
\midrule
Main Model     & 0.854          & 0.833          & \textbf{0.906} & \textbf{0.868} \\
Transfer Model & \textbf{0.856} & \textbf{0.875} & 0.849 & 0.862 \\
\midrule
$\text{abs}(\text{Difference}) $ & 0.002 & 0.042 & 0.057 & 0.006 \\
\bottomrule
\end{tabular}
\label{tab:model_comparison}
\end{table}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/confusion_matrix.png} % Image filename
    \centering
    \caption{Confusion Matrix of main model and transfer learning model} % Caption
    \label{fig:confusion_matrixes} % Label
\end{figure}

 \Cref{tab:model_comparison} highlights the key performance indicators for each model. It can be seen that the EfficientNet transfer learning model and the main model have very comparable performance - with the EfficientNet model and main model achieving an accuracy of 85.6\% and 85.4\% respectively - a difference of only 0.2\%. Furthermore the EfficientNet and main model achieved F1 scores of 0.862 and 0.868 respectively, a difference of only 0.006.

Where these models differ is their precision and recall. The EfficientNet model achieved a precision of 87.5\% (4.2\% higher than the main models' 83.3\%.) The main model achieved a recall of 90.6\% (5.7\% higher than the EfficientNet models' 84.9\%.) This means that the EfficientNet model is better at minimising false positive AI detection, whereas the main model is better at detecting AI cases without missing them (i.e. reducing false negatives). This is evident in \cref{fig:confusion_matrixes}, where the EfficientNet model has 32 false posiives compared to the main models' 48. Similarly, the main model has only 25 false negatives, compared to the EfficientNet models' 40.

\newpage

\section{AWS Sagemaker Information and Discussion} \label{sec:aws_reflection}

This project was developed using AWS SageMaker. \Cref{fig:jupyter_lab_space_config} shows the JupyterLab environment configuration used. The smallest available instance type, \code{ml.t3.medium}, was selected for the notebook environment, as model training was offloaded to separate training jobs with dedicated compute resources. Each training job used the \code{ml.c5.2xlarge} instance type, chosen for its higher RAM and CPU resources whilst still being cost effective.

\Cref{fig:file_top,fig:file_src} shows the file structure used in the project, as run within the JupyterLab environment. The model definition code (\code{src/model_def.py} and \code{src/transfer_model.py}) and the training scripts (\code{src/train.py} and \code{src/train_transfer.py}) were separated from the main notebooks to improve modularity and reusibility for any case where another notebook would need to call upon these models' structure.


\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/jupyter_lab_space_config.png} % Image filename
    \centering
    \caption{Jupyter lab space configuration screenshot} % Caption
    \label{fig:jupyter_lab_space_config} % Label
\end{figure}

\begin{figure}[htbp]
    \centering
    \begin{minipage}{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/file_structure_top.png}
        \caption{Top level file structure screenshot}
        \label{fig:file_top}
    \end{minipage}\hfill
    \begin{minipage}{0.3\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/file_structure_src.png}
        \caption{Files inside the \code{src} directory screenshot}
        \label{fig:file_src}
    \end{minipage}\hfill
    \begin{minipage}{0.35\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/aws_cost_summary.png} % Image filename
        \caption{Project AWS cost summary screenshot} % Caption
        \label{fig:aws_cost_summary} % Label
    \end{minipage}\hfill
\end{figure}


\Cref{fig:aws_cost_summary} shows the total cost summary for the project, which remained under \$15. This was achieved through careful resource management within a known budget of \$50. General-purpose \code{ml.c5.2xlarge} instances were selected for both training and inference, avoiding more expensive GPU backed instances. While GPU instances would have significantly reduced training time due to their parallel computing capabilities, well suited to machine learning workloads the increased cost was not justified within the project constraints. This tradeoff resulted in longer training times but was acceptable given the budget, and the workflow remains easily extensible to a GPU-backed configuration in a real world deployment. The main model took 56 minutes to train, while the transfer learning model required 67 minutes. Both were trained for 3 epochs on the same \code{ml.c5.2xlarge} instance type.

An issue encountered during the project was related to model deployment. Specifically, the models were deployed as real time endpoints, operating on a server and accessed via HTTP requests. While this allowed the endpoints to be invoked at any time by users with appropriate permissions, it also imposed a payload limit, restricting each request to a maximum of four $500 \times 500$ RGB images. This made the evaluation process slower. As a result, the holdout set was limited to 500 images, since the evaluation workflow was not scalable. Due to limited experience with AWS, configuring a more appropriate batch inference or transformer endpoint proved difficult, and the real time endpoint was used as a compromise.

Due to limited experience with SageMaker and a highly cost conscious mentality, memory issues were encountered during training of the transfer learning model. As a result, the following compromises were made:
\begin{itemize}
    \item reducing the batch size of the transfer learning model,
    \item downsampling images to a resolution of $224 \times 224$,
    \item selecting the EfficientNetB0 architecture instead of higher-capacity alternatives such as EfficientNetB1 or EfficientNetB2.
\end{itemize}
these issues could be very easily addressed with more resources, particularly given that the way in which the model definition code and training scripts were structured in a modular way.


\section{Final Model Discussion and Conclusion}

This report proposed two models to detect if an image is generated using ai - one model developed through hyperparameter tuning, and the other through transfer learning from EfficientNet. The results indicate that both models perform comparably, with slight differences in precision and recall.

The original research objective was to make a tool to distinguish between AI-generated and real images, ultimately as a means to identify the spread of misinformation and other harms in online discourse. Were this model to be deployed as a 'first step' with the intention of more thoughrough investigation, it would be more beneficial to be relaxed on false positives, and minimise false negatives, given the false positives would be vindicated with further analysis or research.

For this reason, under the current configuration, the main model is the preferred option, as it achieves a higher recall (90.6\% vs 84.9\%) and is therefore slightly better at detecting AI-generated images without missing them. However, as discussed in \cref{sec:aws_reflection}, with increased resources, the transfer learning model could be trained at full resolution using a higher capacity architecture such as EfficientNetB2. This would likely improve its performance further, surpassing the main model in both precision and recall, given that it performs comparably even under the current constraints.

While this project achieved comparable accuracy (85.4\% and 85.6\% on the main and transfer learning models respectively) to that reported in \cite{bird2023cifakeimageclassificationexplainable}, which reached nearly 93\% using CNNs and the CIFAKE dataset, it is important to acknowledge that the performance demonstrated here does not reflect research-level rigour. The CIFAKE dataset introduced in that study is a large and diverse benchmark designed to support broader generalisation and deeper analysis across model types. Due to limited experience and project constraints, this dataset was not fully utilised, and evaluation was instead conducted on a smaller, custom-generated holdout set. As such, while the results achieved in this project are competitive within scope, further development would be required to ensure a proper comparison with comparable approaches in the literature.

Overall, it can be concluded that the models developed in this investigation are effective at distinguishing AI-generated images from real ones, with two models successfully trained and deployed to perform this task.
