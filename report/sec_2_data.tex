\newpage

\section{Dataset}

The dataset used for this project was a competition dataset from Hugging Face, held in 2023 \cite{huggingface_competitions_aiornot}.
The dataset consists of 62,060 images, and is 2.37GB in size, being pre-split into training and tesing sets, as summarised in \cref{tab:dataset_summary,tab:class_counts}, where it can be seen that the testing set has the class labels withheld due to the competition setting, restricting this analysis to the 18,618 training images, which we can sub-divide and validate with known labels.

\begin{table}[h]
    \centering
    \begin{tabular}{ll}
        \toprule
        \textbf{Feature} & \textbf{Description} \\
        \midrule
        \code{id}     & Index filename \code{34.jpg} \\
        \code{image}  & The Image object (rgb 512x512 resolution) \\
        \code{label}  & Binary class label [1=AI, 0=not AI] \\
        \bottomrule
    \end{tabular}
    \caption{Dataset features and their descriptions.}
    \label{tab:dataset_summary}
\end{table}

\begin{table}[h]
    \centering
    \begin{tabular}{lcc}
        \toprule
        \textbf{Class Label} & \textbf{Train Count} & \textbf{Test Count} \\
        \midrule
        AI (1)      & 10,330 $(55.5\%)$ & NA \\
        Not AI (0)  & 8,288 $(45.5\%)$ & NA \\
        \bottomrule
        \textbf{Total}       & 18,618 & 43,442 \\
    \end{tabular}
    \caption{Counts of each class label in the training and testing sets, where it can be seen that the testing set has the class labels withheld due to the competition setting}
    \label{tab:class_counts}
\end{table}

\Cref{code:data_wrangling} shows the code used to load the dataset, and split it into training, validation, and test sets \note{have a look at this later} \note{sub splitting of the data was reasoned to be the best strategy, as having a smaller dataset on which to train, \code{small_train}, it would be more conducive to iteration in the hyperparameter tuning process, and the optimal hyperparameters could then be transferred onto the main model, which would be trained on the full training dataset}. The code demonstrates how the data is imported. Since Hugging Face datasets are not natively compatible with TensorFlow, the dataset is converted to a TensorFlow dataset using the \texttt{to\_tf\_dataset()} method. 

\lstinputlisting[language=Python, caption=Data Wrangling Script \note{NEED TO CHANGE}, label=code:data_wrangling]{code_chunks/sec_2_data_wrangling.py}

This data was converted to npz files, due to their efficient storage and loading capabilities. After conversion, the data was uploaded to an S3 bucket for easy access during model training and evaluation. Uploading to S3 buckets was critical, as it meant that there was no longer a reliance on the RAM allocation of the sagemaker notebook environment - it could be loaded in from the S3 bucket in batches, which has the benefit of being a more scalable solution.
